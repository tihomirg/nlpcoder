Title: "Programming using API keywords" (expression repair)

0) Holes we solve with types:
  - locals form the input (we may include literals as well here) 
  - other locals
  
  Hole handler relays on types. Since we have a small number 
  of locals we can form a data structure that collects them and 
  store them based on their compatible types. Also variable type 
  is compatible with all holes.
  
  if variable, than all locals are suitable, else, only some, from the
  data structure. Watch out 
  
1) Literals:
  - literal from the input
  - from the model. We create them using extra information. *

2) Tweaking:
  - put into distinct groups
  - tweaking more input parsing, sometimes duplicate tokens appear in the input, resolve this 
  - try without pos
  
3) Solve arrays and numerals
4) Collect new statistics
5) print/println, and + operator, try to cover at least this one
6) WordNet
7) JavaDoc

0) One extra filed in PartialExpressionStatistics that denotes a number of connections. The closer to this number we are the better.
If we have more connections in PartialExpression we start subtracting a reward.

1) Tweaking:
  Now:
   Literals:
    - Seems that word string is specific.
    - Include literals, merge with 
    - More sophisticated scoring algorith, that favors correct args.
    - We need a mechanism that eliminates cycles :)
    
  Search:
   - Work on the parser processing more. Some words tokens should not appear twice in the groups.
   - Find a good ration between decl frequency and word scoring
   - Think about excluding "pos" when matching declarations. Use only words.
  
  Synthesis:
   - Check if everything is ok with "connections checking".
   - Include additional scores for connections and for the correct connections.
     (find the best ratio between match/decl-frequencies/connection)
   - Include expressions without connections.
   - Collect "compositional statistics"
   - Include missing groups (literals, and other excluded)
   - Make "initialScore" in ExprGroup works.
   - Make that check if there are 0 groups, we skip merge phase.
   
2) We would need to do STEAMING with WordNet steamer.
3) Should try the entire algorithm without POS. 
We need to thik about possitions. Maybe it is good to get rid of them
For instance "a buffered file" would not be mapped to new BufferedInputStrema(new File()), cause in the first sentence it is adjective and in the second it's a verb.

4) Think about WordNet, and what related words indeed to use. For some tokens, that are literals or locals we do not need related words.
5) Manually: What about lemmatization of "println" --> "print new line" //we should get this from the JavaDoc
6) Include JavaDoc.
7) Change "locals" with their return type
8) Extract/Generate argument constraints
9) Use argument constraints:
   - add also constraint that between the argument of "the hole" is the list of "non-constraint declarations". This way, all declarations are constraint with
   at least one constraint. Also, this would mean that we have an idea what declarations should be head declarations of the final expression.

10) Numeric literals
11) When and if "map from supertype to list of subtypes" (this is similar to words) we can keep it in the
    - we should implement Object as separate class. 

Done:

1) Do word lemmatization. //Still see if we need WordNet lemmatizer/steammer.
2) Unique declarations
3) Keep receiver tokens separated.
4) Connect and test.
5) Build word selection groups.
6) Implement missing methods in selection.
7) Clean selection table after you print/copy top statistics.

8) Remaining make trouble. Obviously we need to think in terms of groups of words in declaration. Also use sets where needed.
For instance: if a declaration has [name words]:[remaining words]:[receiver words] => [multiset]:[multiset]:[singleton set]

9) Technique that maximizes the hit. Given words that hit a declaration we try to maximize the hit. i.e. cover as many distinct positions in the declaration.
The most important is the name. Others must not multiply, especially, remaining receiver words.

Again we need groups. Now we can introduce scorers for hits (1,1), (2,2), (1,2), (2,1)....bases should be maximize hit tactics.
This means if one word has an option to hit many places it takes into account the other hits and together they maximize the hit.
Also, we need to take into account places, and missed places as well. Two args with the same type shouldn't be hit two times by one word.
Only the one that brings more "points" will be hit. The other one needs to wait for more words to come. We get points based on the
declaration structure not based on the word hit. JavaDoc can be used for main group, name hit.

10) Word importance measured in calculus. For instance if f("file") > f("input") this means that those that we select with "input" will have more importance than
those selected with "file"

11) Simple max algorithm:
    - sort slots by importance
    - check if the declaration slot is free
    - if yes, occupy the slot
    - if not, check if it can bring higher score than the current.
    - if yes, substitute and continue wit the substitute (in substitutes direction)
    - if not, continue it the current direction 
    
12) Remove "abstract" constructors!
13) Change "literals" with their return types  

14) Same holds for "int" -> "integer", "bool" -> "boolean" and so on. We should make our own list.
  - can be solver by getting a boxed type and its wordset.  
  
15) Included declaration frequencies